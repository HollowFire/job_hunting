	首先是提示词的自动生成功能，它是根据用户提供的任务描述，直接生成一个完整的系统prompt。这个功能在当前很多agent平台上都有提供，基本上都是利用大模型来理解用户的需求，通过填充一个提示词元模板的框架，来生成一个完整的系统prompt，这个框架中可能会包含角色描述、技能描述、执行步骤、输出结构、约束条件等等这样一些元素。但是我们发现，并不是所有的场景都适用同样一个模板，例如在文案撰写场景下，输出格式可能是一个比较重要的元素，但是如果我们需要创建一个智能体来帮我们调用各种工具，那如果prompt中出现了输出格式的相关约束，就会直接影响调用工具的效果。因此我们对用户的任务场景做了细分，划分出了推理类、角色扮演类、文案撰写类、（计划制定类、工具调用类、信息分析类）这一系列的类别，每个类别的元模板中都包含不同的元素种类。我们的prompt生成算法在运行时，就会首先对用户的任务场景进行类别判定，然后路由到相应的元模板，进行prompt的生成。

	在有了提示词生成功能之后，我们还开发了一个提示词自动优化功能，它的目的是帮助开发者从繁琐的提示词优化工程中解脱出来，帮助他们的提示词快速地在测试集上达到目标准确率。Prompt自动优化算法其实在学术界有比较多的相关研究，包括一些文本梯度的方法和遗传变异的算法。我们基于这些算法，再结合我们常见的业务场景和目标，做出了一套自动优化的流程和工具。在用户使用自优化功能时，需要提供一个初始化的prompt，以及一个评测数据集，这个数据集中需要包含一些case，每个case包括用户的输入以及期望的输出，除此之外，用户还需要提供一个评分准则，我们依据这个准则，对大模型的实际输出和参考答案进行对比，从而得到一个打分，这个打分是提示词自动优化过程中的重要依据。我们提供的评分准则包括这么几种，首先是文本的完全匹配，它适用于分类或多项选择这种有确定标签答案的场景。然后就是json匹配它适用于需要大模型进行结构化输出的场景。除此之外，还有基于语意相似度的打分，也就是将大模型输出文本和参考答案文本分别通过一个embedding模型编码成向量，然后将两个向量的cosine相似度作为分数。上面说的三种打分策略，都是利用客观规则或者确定的程序进行打分，它们有各自的适用场景，如果在某些场景下，这些方法都不适用，比如在文案撰写场景，在评判大模型输出的时候，通常是基于一些主观的意见，例如文案的丰富度、结构是否清晰、是否有吸引力等等，针对这些主观评判的场景，我们还提供了一种通用的打分策略，就是利用大模型进行打分，用户可以以文本的形式输入一些评测的维度，例如丰富度、吸引力等等，那么大模型就会参考用户给出的维度，对输出进行打分。上面所说的初始prompt、评测数据集和评分准则都准备好之后，我们的自动优化算法就可以运行起来了。这个算法的运行是一个循环迭代的过程，在每一次迭代中，都会包含预测、打分、优化这样一系列步骤，首先预测过程，就是让目标大模型对评测集中的case进行推理，得到预测答案；然后是打分步骤，它会根据参考答案对预测答案进行打分，得到一个1-5分的分数，这个过程除了输出分数之外，还会包含一些额外信息，比如用户如果选择json匹配打分，那么假使大模型输出的json结构体不完整，导致解析报错，那么这些报错信息会被保存下来，它能够在后续的优化步骤中指导模型进行有针对性的提示词优化；在最后一个优化步骤中，就是基于前面的评估结果进行提示词的优化。这个步骤中，我们对常见的文本梯度和遗传变异两种算法进行了结合。首先，我们基于文本梯度的方法，根据反馈信息对prompt进行修正，并得到多个不同的修正版本，然后我们分别对这些版本的prompt以及他们的交叉融合的版本进行评估，选择效果较好的prompt进行下一轮迭代。经过一定的迭代次数，或者评测准确率达到目标区间之后，算法运行会终止，并将最终的prompt返回给用户。
