1. 为什么decoder-only好？
- 双向注意力容易退化为低秩状态，causal attention必是满秩
- decoder-only架构中每个位置所见信息更少，预测难度更大，在大模型、多数据的情况下，有利于学习更通用的表征
- causal attention具有隐式位置编码

2. PEFT方法分类
- 增量参数微调：adapter、软提示微调
- 选择参数微调：结构化、非结构化
- 重构参数微调：LoRA
- 上述组合

3. 位置编码
- 在sinusoid位置编码中，模型所见的位置编码信息表现为多维数组而非公式中有规律的位置变化，导致其无法捕获位置变化规律，从而对不同的位置信息进行记忆，而非进行编码，论文证明，当推理长度大于训练长度时，模型ppl出现爆炸的现象

4. T5
- 与Transformer结构差异：相对位置编码、无bias的LayerNorm、LayerNorm置于残差之外
- 位置编码：头间各异、层间共享的相对位置编码；最大截断长度128
