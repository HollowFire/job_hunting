1. 为什么decoder-only好？
- 双向注意力容易退化为低秩状态，causal attention必是满秩
- decoder-only架构中每个位置所见信息更少，预测难度更大，在大模型、多数据的情况下，有利于学习更通用的表征
- causal attention具有隐式位置编码

2. PEFT方法分类
- 增量参数微调：adapter、软提示微调
- 选择参数微调：结构化、非结构化
- 重构参数微调：LoRA
- 上述组合

3. 位置编码
- 绝对位置编码，指的是每个位置有一个固定的向量值，该向量值被加在词向量中，参与整个神经网络的向前传播计算。相对位置编码只在需要用到token位置信息的地方才引入位置信息，transformer网络中有两个核心网络层，一个是attention，一个是ffn。ffn是点到点的，与位置不相关，所以只有在attention层中需要用到位置信息。而在我们的直觉中，只有token之间的相对位置才是有意义的，比如同样一个句子的两个单词，当他们出现在文章第一段或者最后一段，他们的绝对位置是不同的，但是不应该去影响它们在相对位置上的关系。
- T5相对位置编码：在query向量与key向量的乘积之上加上一个标量，标量值只和query和key之间的相对位置远近有关。并不是每个m-n都对应一个不同的标量值，0-1对应一个值，2-5对应一个值。并且有截断，当位置差超过128时，它的标量值就不变了
- rope: 是一种相对位置编码的方法，主要用在attention层中，在把query向量与key向量相乘计算attention score的时候，将不同token的位置信息引入到计算过程中。具体的计算过程实际上是把token向量的各个维度两两分组，每组通过一个特定角度的cos和sin值矩阵进行变换，把这个变换过程投射到复数的二维矩阵上，就相当于是对词向量做了一个角度旋转。那具体旋转的角度是和token相对位置的差值相关的，差的越多，旋转角度越大。因此，在训练过程中，模型能够观察到不同的位置差对应的旋转角度给词向量带来的变化，这种变化是有规律的，模型通过学习这种规律，在遇到更长的位置编码时，能够利用这种规律来感知到token之间的位置关系，并把这种位置关系带入到attention的计算中去。
- 外推性：指的是训练时用较短的文本长度，而在推理时使用更长的文本长度。首先绝对位置编码的外推性差，有两种绝对位置编码，一种是可学习的位置编码，它将每个位置one-hot编码转换成一个向量，这个转换过程是通过一个矩阵来映射，矩阵是通过训练学习的，所以它天然无法外推，因为没法把更长的位置编码到一个事先固定维度的one-hot编码上。还有一种是三角函数的位置编码，理论上它可以将更长的位置信息编码成相应的位置编码向量，但是它的转换公式使得它编码得到的位置向量对应的多维数组缺乏一些直观的规律，对于模型来说，他只能通过拟合来记忆已见过的位置信息，但是遇到更远的位置信息时，它无法推理出一个合理的有规律的位置编码值，因此不具备外推性，当推理长度大于训练长度时，出现ppl爆炸的现象。

4. T5
- 与Transformer结构差异：相对位置编码、无bias的LayerNorm、LayerNorm置于残差之外
- 位置编码：头间各异、层间共享的相对位置编码；最大截断长度128

5. Agent构建
- 当前的agent平台提供了单prompt的自动构建功能，实际效果可能不如有一定经验的人手写prompt
- 未来的agent构建是需要不同角色的联结，即多agent构建。达到两个效果：构建不同的agent角色和他们的联结逻辑；允许用户修改单个角色的职责功能，并自动进行上下游角色修正

6. DPO vs PPO
- https://mp.weixin.qq.com/s/mhPJzhQvPJlAWsO2nW9BHg

7. Quantization & Overflow
- https://blog.csdn.net/qq_43799400/article/details/134182459
- https://zhuanlan.zhihu.com/p/657886517?theme=dark
- fp32(1+8+23), fp16(1+5+10, 精度0.001), bf16(1+8+7, 精度0.01)

8. GPT-o1
- 强化学习（Let's verify step by step），结果监督（通过针对数学、变成问题的编译或验证器）模型、中间步骤监督模型
- 推理过程的隐藏CoT
- 在普通的文本编辑、撰写、语言风格转换上劣于GPT-o模型
